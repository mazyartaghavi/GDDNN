import numpy as np

# Define the activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Define the loss function and its derivative
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def mean_squared_error_derivative(y_true, y_pred):
    return 2 * (y_pred - y_true) / len(y_true)

class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
        self.weights = []
        self.biases = []

        # Initialize the weights and biases randomly
        for i in range(1, len(layers)):
            W = np.random.randn(layers[i], layers[i-1])
            b = np.zeros((layers[i], 1))
            self.weights.append(W)
            self.biases.append(b)

    def forward_propagation(self, X):
        activations = [X]
        zs = []

        for i in range(len(self.layers) - 1):
            W = self.weights[i]
            b = self.biases[i]
            z = np.dot(W, activations[-1]) + b
            a = sigmoid(z)
            zs.append(z)
            activations.append(a)

        return activations, zs

    def backward_propagation(self, X, y, activations, zs):
        num_examples = X.shape[1]
        gradients_w = [np.zeros(W.shape) for W in self.weights]
        gradients_b = [np.zeros(b.shape) for b in self.biases]
        delta = mean_squared_error_derivative(y, activations[-1]) * sigmoid_derivative(zs[-1])
        gradients_w[-1] = np.dot(delta, activations[-2].T) / num_examples
        gradients_b[-1] = np.mean(delta, axis=1, keepdims=True)

        for l in range(2, len(self.layers)):
            delta = np.dot(self.weights[-l+1].T, delta) * sigmoid_derivative(zs[-l])
            gradients_w[-l] = np.dot(delta, activations[-l-1].T) / num_examples
            gradients_b[-l] = np.mean(delta, axis=1, keepdims=True)

        return gradients_w, gradients_b

    def update_parameters(self, gradients_w, gradients_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * gradients_w[i]
            self.biases[i] -= learning_rate * gradients_b[i]

    def train(self, X, y, learning_rate, num_epochs, batch_size):
        num_examples = X.shape[1]

        for epoch in range(num_epochs):
            # Shuffle the data
            indices = np.random.permutation(num_examples)
            X = X[:, indices]
            y = y[:, indices]

            for i in range(0, num_examples, batch_size):
                # Select a mini-batch
                X_batch = X[:, i:i+batch_size]
                y_batch = y[:, i:i+batch_size]

                # Perform forward propagation
                activations, zs = self.forward_propagation(X_batch)

                # Perform backward propagation
                gradients_w, gradients_b = self.backward_propagation(X_batch, y_batch, activations, zs)

                # Update the parameters
                self.update_parameters(gradients_w, gradients_b, learning_rate)

    def predict(self, X):
        activations, _ = self.forward_propagation(X)
        return activations[-1]

# Example usage
# Assume we have a dataset X and corresponding labels y
X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([[0], [1], [1]])

# Define the network architecture
layers = [2, 3, 1]

# Initialize and train the neural network
learning_rate = 0.1
num_epochs = 1000
batch_size = 3

model = NeuralNetwork(layers)
model.train(X.T, y.T, learning_rate, num_epochs, batch_size)

# Make predictions
predictions = model.predict(X.T)
print("Predictions:", predictions)
